import os
import sys
import subprocess
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from pathlib import Path
import tempfile
from tqdm import tqdm

# -------------------------------
# Configuration
# -------------------------------
DEMO_FOLDER = "DEMO"
PREPROCESSED_FOLDER = "DEMO/preprocessed_frames-demo"
EMBEDDINGS_FOLDER = "DEMO/embeddings-demo"
MODEL_PATH = "temporal_cnn_dino_125.pth"
NUM_FRAMES = 125
EMB_DIM = 384
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Occlusion settings
OCCLUSION_WINDOW = 5  # odd number recommended

# -------------------------------
# Temporal CNN Model
# -------------------------------
class TemporalCNN(nn.Module):
    def __init__(self, emb_dim=EMB_DIM, num_classes=2):
        super().__init__()

        self.temporal = nn.Sequential(
            nn.Conv1d(emb_dim, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),

            nn.Conv1d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),

            nn.AdaptiveAvgPool1d(1)
        )

        self.classifier = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.transpose(1, 2)  # (B, T, D) ‚Üí (B, D, T)
        x = self.temporal(x)
        x = x.squeeze(-1)
        return self.classifier(x)


# -------------------------------
# Temporal Occlusion
# -------------------------------
def temporal_occlusion_curve(model, embeddings, window):
    model.eval()
    T = embeddings.shape[0]
    mean_embedding = embeddings.mean(axis=0)

    with torch.no_grad():
        base_x = torch.tensor(embeddings, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        base_prob = torch.softmax(model(base_x), dim=1)[0, 1].item()

    drops = np.zeros(T)
    half = window // 2

    for t in range(T):
        occluded = embeddings.copy()
        start = max(0, t - half)
        end = min(T, t + half + 1)

        occluded[start:end] = mean_embedding

        with torch.no_grad():
            x = torch.tensor(occluded, dtype=torch.float32).unsqueeze(0).to(DEVICE)
            prob = torch.softmax(model(x), dim=1)[0, 1].item()

        drops[t] = base_prob - prob

    return drops, base_prob

# -------------------------------
# Plotting Utilities
# -------------------------------
def plot_temporal_anomaly(video_name, drops, save_dir):
    frames = np.arange(len(drops))
    drops_norm = drops / (np.max(drops) + 1e-8)

    plt.figure(figsize=(12, 4))
    plt.plot(frames, drops, linewidth=2)
    plt.xlabel("Frame Index")
    plt.ylabel("ŒîP(fake)")
    plt.title(f"Temporal Occlusion Anomaly Curve\n{video_name}")
    plt.grid(True)

    out_path = os.path.join(save_dir, f"{video_name}_temporal_occlusion.png")
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()

    # Heatmap-style visualization
    plt.figure(figsize=(12, 1.5))
    plt.imshow(drops_norm[np.newaxis, :], aspect="auto", cmap="hot")
    plt.yticks([])
    plt.xlabel("Frame Index")
    plt.title("Normalized Temporal Contribution Heatmap")
    plt.colorbar(label="Relative Contribution")

    heatmap_path = os.path.join(
        save_dir, f"{video_name}_temporal_occlusion_heatmap.png"
    )
    plt.tight_layout()
    plt.savefig(heatmap_path, dpi=200)
    plt.close()

    return out_path, heatmap_path


# -------------------------------
# Main Pipeline
# -------------------------------
def main():
    print("=" * 60)
    print("üé¨ DEMO Video Prediction Pipeline")
    print("=" * 60)
    print(f"üìÅ Demo folder: {DEMO_FOLDER}")
    print(f"üñ•Ô∏è  Device: {DEVICE}")
    print(f"üìä Model: {MODEL_PATH}")
    print("=" * 60)
    print()

    # Step 0: Check if DEMO folder exists and has videos
    if not os.path.exists(DEMO_FOLDER):
        print(f"‚ùå Error: DEMO folder '{DEMO_FOLDER}' not found!")
        print(f"   Please create the folder and add video files to it.")
        sys.exit(1)
    
    # Check for video files
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']
    video_files = set()  # Use set to avoid duplicates
    for ext in video_extensions:
        video_files.update(Path(DEMO_FOLDER).glob(f"*{ext}"))
        video_files.update(Path(DEMO_FOLDER).glob(f"*{ext.upper()}"))
    video_files = list(video_files)  # Convert back to list
    
    if len(video_files) == 0:
        print(f"‚ùå Error: No video files found in '{DEMO_FOLDER}' folder!")
        print(f"   Supported formats: {', '.join(video_extensions)}")
        print(f"   Please add video files to the DEMO folder and run again.")
        sys.exit(1)
    
    print(f"‚úÖ Found {len(video_files)} video(s) in DEMO folder")
    print()

    # Step 1: Check if model exists
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Error: Model file '{MODEL_PATH}' not found!")
        print("   Please train the model using train_temporal_cnn.py first.")
        sys.exit(1)
    
    print(f"‚úÖ Model file found: {MODEL_PATH}")
    print()

    # Step 2: Preprocess videos from DEMO folder (direct processing)
    print("üîÑ Step 1/3: Preprocessing videos...")
    print("-" * 60)
    
    # Create output directory
    os.makedirs(PREPROCESSED_FOLDER, exist_ok=True)
    
    # Preprocessing configuration (match training settings)
    frame_rate = 25
    duration = 5
    resolution = 224
    resize_method = "bicubic"
    image_format = "png"
    
    processed_count = 0
    for video_idx, video_path in enumerate(tqdm(sorted(video_files), desc="  Preprocessing"), start=1):
        video_name = video_path.stem
        output_dir = os.path.join(PREPROCESSED_FOLDER, f"video_{video_idx}")
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract frames directly using FFmpeg
        # Sample uniformly across the video
        total_frames = duration * frame_rate
        
        for frame_idx in range(NUM_FRAMES):
            # Calculate timestamp for uniform sampling
            timestamp = (frame_idx / NUM_FRAMES) * duration
            output_frame = os.path.join(output_dir, f"frame_{frame_idx:04d}.{image_format}")
            
            # Build FFmpeg command for single frame extraction with preprocessing
            # Use force_original_aspect_ratio=increase to handle both portrait and landscape
            # This scales so the smaller dimension becomes 224, then crops to 224x224 (center)
            ffmpeg_cmd = [
                "ffmpeg", "-y", "-loglevel", "error",
                "-ss", str(timestamp),
                "-i", str(video_path),
                "-vf", f"fps={frame_rate},scale={resolution}:{resolution}:force_original_aspect_ratio=increase:flags={resize_method},crop={resolution}:{resolution}",
                "-vframes", "1",
                "-pix_fmt", "rgb24",
                output_frame
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
            except subprocess.CalledProcessError:
                pass  # Skip failed frames
        
        # Verify we extracted frames
        extracted = len([f for f in os.listdir(output_dir) if f.endswith(f".{image_format}")])
        if extracted > 0:
            processed_count += 1
    
    print(f"\n‚úÖ Preprocessing complete! Processed {processed_count}/{len(video_files)} videos")
    print()

    # Step 3: Extract embeddings using a temporary script
    print("üîÑ Step 2/3: Extracting embeddings...")
    print("-" * 60)
    
    # Create temporary extraction script
    extract_script = f"""
import os
import sys
import numpy as np
import torch
from transformers import AutoModel, AutoImageProcessor
from PIL import Image
from pathlib import Path
from tqdm import tqdm

FRAMES_ROOT = "{PREPROCESSED_FOLDER}"
OUTPUT_FOLDER = "{EMBEDDINGS_FOLDER}"
MODEL_NAME = "facebook/dinov3-vits16-pretrain-lvd1689m"
IMAGE_FORMAT = "png"
DEVICE = "{DEVICE}"

print("Loading DINOv3 model...")
processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
model.eval()
print("‚úÖ Model loaded successfully!")
print()

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def extract_embeddings_from_frames(frame_dir, image_format="png"):
    frame_files = sorted(Path(frame_dir).glob(f"frame_*.{{image_format}}"))
    if len(frame_files) == 0:
        raise ValueError(f"No frames found in {{frame_dir}}")
    
    embeddings = []
    for frame_file in frame_files:
        img = Image.open(frame_file).convert("RGB")
        inputs = processor(images=img, return_tensors="pt").to(DEVICE)
        
        with torch.no_grad():
            outputs = model(**inputs)
            cls_emb = outputs.last_hidden_state[:, 0, :]
        
        embeddings.append(cls_emb.cpu().numpy())
    
    embeddings = np.vstack(embeddings)
    return embeddings

# Process all video directories
if not os.path.exists(FRAMES_ROOT):
    print(f"Error: Preprocessed frames folder not found: {{FRAMES_ROOT}}")
    sys.exit(1)

video_dirs = [d for d in os.listdir(FRAMES_ROOT) if os.path.isdir(os.path.join(FRAMES_ROOT, d))]
print(f"Found {{len(video_dirs)}} video(s) to process")
print()

if len(video_dirs) == 0:
    print("Warning: No video directories found in preprocessed frames folder")
    sys.exit(1)

for video_dir in tqdm(video_dirs, desc="Extracting embeddings"):
    frame_dir_path = os.path.join(FRAMES_ROOT, video_dir)
    try:
        emb = extract_embeddings_from_frames(frame_dir_path, IMAGE_FORMAT)
        save_name = f"{{video_dir}}.npy"
        np.save(os.path.join(OUTPUT_FOLDER, save_name), emb)
    except Exception as e:
        print(f"\\nWarning: Skipping {{video_dir}}: {{e}}")

print()
print("‚úÖ Embedding extraction complete!")
"""
    
    # Write to temporary file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
        temp_script_path = f.name
        f.write(extract_script)
    
    try:
        # Use same UTF-8 environment for embedding extraction
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        result = subprocess.run(["python", temp_script_path], capture_output=True, text=True, env=env)
        print(result.stdout)
        
        if result.returncode != 0:
            print(f"‚ùå Error during embedding extraction:")
            print(result.stderr)
            sys.exit(1)
    finally:
        # Clean up temporary file
        if os.path.exists(temp_script_path):
            os.remove(temp_script_path)
    
    print()

    # Step 4: Load model and make predictions with temporal occlusion analysis
    print("üîÑ Step 3/3: Making predictions with temporal explainability...")
    print("-" * 60)
    
    # Load the trained model
    model = TemporalCNN().to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()
    print(f"‚úÖ Model loaded from {MODEL_PATH}")
    print()

    # Get all embedding files
    if not os.path.exists(EMBEDDINGS_FOLDER):
        print(f"‚ùå Embeddings folder not found: {EMBEDDINGS_FOLDER}")
        sys.exit(1)
    
    embedding_files = sorted([f for f in os.listdir(EMBEDDINGS_FOLDER) if f.endswith(".npy")])
    
    if len(embedding_files) == 0:
        print(f"‚ùå No embeddings found in {EMBEDDINGS_FOLDER}")
        sys.exit(1)

    print(f"üìä Found {len(embedding_files)} video(s) to classify")
    print("=" * 60)
    print()

    # Make predictions with temporal occlusion analysis
    for emb_file in embedding_files:
        emb_path = os.path.join(EMBEDDINGS_FOLDER, emb_file)
        video_name = emb_file.replace(".npy", "")
        
        try:
            # Load embeddings
            embeddings = np.load(emb_path)
            
            # Validate shape
            if embeddings.shape != (NUM_FRAMES, EMB_DIM):
                print(f"‚ö†Ô∏è  Warning: Unexpected embedding shape for {video_name}: {embeddings.shape}")
                print(f"   Expected: ({NUM_FRAMES}, {EMB_DIM})")
                print(f"   Skipping this video...")
                print()
                continue
            
            # Convert to tensor and add batch dimension
            x = torch.tensor(embeddings, dtype=torch.float32).unsqueeze(0).to(DEVICE)
            
            # Make prediction on entire video
            with torch.no_grad():
                output = model(x)
                probabilities = torch.softmax(output, dim=1)
                prediction = torch.argmax(output, dim=1).item()
                confidence = probabilities[0, prediction].item()
            
            # Get probabilities for both classes
            prob_real = probabilities[0, 0].item()
            prob_fake = probabilities[0, 1].item()
            
            # Determine label
            label = "GENERATED/DEEPFAKE" if prediction == 1 else "REAL"
            
            # Print overall results with nice formatting
            print(f"üìπ Video: {video_name}")
            print(f"   üéØ Prediction: {label}")
            print(f"   üíØ Confidence: {confidence * 100:.2f}%")
            print(f"   üìä Probabilities:")
            print(f"      ‚Ä¢ Real: {prob_real * 100:.2f}%")
            print(f"      ‚Ä¢ Generated/Deepfake: {prob_fake * 100:.2f}%")
            
            # Perform temporal occlusion analysis
            print(f"\n   üîç Temporal Occlusion Analysis:")
            print(f"   {'-' * 50}")
            
            drops, base_prob = temporal_occlusion_curve(
                model, embeddings, OCCLUSION_WINDOW
            )
            
            # Save raw occlusion curve data
            np.save(
                os.path.join(EMBEDDINGS_FOLDER, f"{video_name}_temporal_occlusion.npy"),
                drops
            )
            
            # Generate plots
            curve_path, heatmap_path = plot_temporal_anomaly(
                video_name, drops, EMBEDDINGS_FOLDER
            )
            
            print(f"   üìà Plots saved:")
            print(f"      ‚Ä¢ {curve_path}")
            print(f"      ‚Ä¢ {heatmap_path}")
            
            # Show top influential frames
            top_idx = np.argsort(drops)[::-1][:10]
            print(f"\n   ‚ö†Ô∏è  Top 10 Most Influential Frames (highest contribution to detection):")
            for i, frame_num in enumerate(top_idx, 1):
                print(f"      {i:2d}. Frame {frame_num:3d}  (ŒîP = {drops[frame_num]:.4f})")
            
            # Show summary statistics
            avg_drop = np.mean(drops)
            max_drop = np.max(drops)
            min_drop = np.min(drops)
            
            print(f"\n   üìà Frame Statistics:")
            print(f"      ‚Ä¢ Average contribution: {avg_drop:.4f}")
            print(f"      ‚Ä¢ Maximum contribution: {max_drop:.4f}")
            print(f"      ‚Ä¢ Minimum contribution: {min_drop:.4f}")
            print(f"      ‚Ä¢ Frames analyzed: {NUM_FRAMES}")
            print()
            
        except Exception as e:
            print(f"‚ùå Error processing {video_name}: {e}")
            print()

    print("=" * 60)
    print("üéâ Prediction complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()
